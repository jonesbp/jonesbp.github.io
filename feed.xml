<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/">
<channel>
<title>La Tinaja</title>
<description>My name is Brian Jones. These are my notes on topics including books, publishing, tech, and history.</description>
<link>http://tinaja.computer</link>
<atom:link href="http://tinaja.computer/feed.xml" rel="self" type="application/rss+xml" />

<item>
<title>Explaining the Obvious</title>

<description>
    <![CDATA[
    <blockquote>
  <p>Be more explicit than you think is necessary, and you’ll nip misunderstandings in the bud. When everyone’s in the same room, it’s easier to notice an errant gesture that shows we’re not all on the same page. A remote team can only know what you say and type, so get everyone to agree that overexplaining won’t be taken as an insult. Don’t let it slip as the project goes on, either. “So what we’re saying is…” should be a common phrase throughout the life of the product.</p>

  <p>– <cite>Drew Bell on <a href="https://trackchanges.postlight.com/five-ways-to-maintain-a-strong-remote-product-team-5d430c164ddf">talking through the “obvious” on remote teams</a></cite></p>
</blockquote>

    
    ]]>
</description>
<pubDate>Thu, 09 Nov 2017 07:39:01 -0500</pubDate>
<link>http://tinaja.computer/2017/11/09/explain-the-obvious.html</link>
<guid isPermaLink="true">http://tinaja.computer/2017/11/09/explain-the-obvious.html</guid>
</item>

<item>
<title>Open a File in MultiMarkdown Composer from BBEdit</title>

<description>
    <![CDATA[
    <p>Sometimes I find myself doing more extensive writing in a Markdown file in BBEdit than I had anticipated and want to switch to a more focused environment. My preferred Markdown editing environment for longer writing sessions is MultiMarkdown Composer, so I’ve added an AppleScript to BBEdit’s scripts menu to open whatever file I’m currently working on in with that. The script follows and, as you will see, can be easily edited to work with any other text editor you prefer.</p>

<div class="language-applescript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">tell</span><span class="w"> </span><span class="nb">application</span><span class="w"> </span><span class="s2">"BBEdit"</span><span class="w">
    </span><span class="k">set</span><span class="w"> </span><span class="nv">markdownFile</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="nv">file</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="nb">front</span><span class="w"> </span><span class="na">window</span><span class="w">
</span><span class="k">end</span><span class="w"> </span><span class="k">tell</span><span class="w">

</span><span class="k">tell</span><span class="w"> </span><span class="nb">application</span><span class="w"> </span><span class="s2">"MultiMarkdown Composer"</span><span class="w">
    </span><span class="nb">open</span><span class="w"> </span><span class="nv">markdownFile</span><span class="w">
    </span><span class="nb">activate</span><span class="w">
</span><span class="k">end</span><span class="w"> </span><span class="k">tell</span><span class="w">
</span></code></pre></div></div>

<p>That’s it.</p>

<p>Save the file in <code class="highlighter-rouge">~/Library/Application Support/BBEdit/Scripts</code>. You can get to this folder from the Finder by holding the ‘Option’ key while selecting the ‘Go’ menu in order to expose the ‘Library’ item which is usually hidden. A new item with the name of whatever you’ve named your script file will now appear in BBEdit’s script menu (identified by the AppleScript paper scroll icon).</p>

<p>You can add another layer of convenience by adding a keyboard shortcut for this action using the ‘Keyboard’ pane in the System Preferences application. Having opened the ‘Keyboard’ pane, go to the ‘Shortcuts’ tab and select ‘App Shortcuts’ from the sidebar. Click the ‘+’ button below the list on the right side and select ‘BBEdit’ as the Application, type your menu’s title exactly as it appears in the application, and set your keyboard shortcut. I use <code class="highlighter-rouge">Shift+Cmd+M</code>.</p>

    
    ]]>
</description>
<pubDate>Thu, 02 Nov 2017 11:46:01 -0400</pubDate>
<link>http://tinaja.computer/2017/11/02/multimarkdown-composer-from-bbedit.html</link>
<guid isPermaLink="true">http://tinaja.computer/2017/11/02/multimarkdown-composer-from-bbedit.html</guid>
</item>

<item>
<title>Athanasius Kircher’s Cat Piano</title>

<description>
    <![CDATA[
    <blockquote>
  <p>Not so long ago, in order to dispel the melancholy of some great prince, a noted and ingenious actor constructed an instrument such as this. He took live cats all of different sizes, and shut them up in a kind of box especially made for this business, so that their tails, stuck through the holes, were inserted tightly into certain channels. Under these he put keys fitted with the sharpest points instead of mallets. Then he arranged the cats tonally according to their different sizes, so that each key corresponded to the tail of one cat, and he put the instrument prepared for the relaxation of the prince in a suitable place. Then when it was played, it produced such music as the voices of cats can produce. For when the keys, depressed by the fingers of the organist, pricked the tails of the cats with their points, they, driven to a rage, with miserable voices, howling now low, now high, produced such music made of the voices of the cat as would move men to laughter and even arouse shrews to dance.</p>

  <p>– <cite>Athanasius Kircher in <em>Musurgia Universalis</em> (1650), as translated by Frederick Baron Crane, as printed on the insert in the last <a href="https://publicdomainreview.org/support/">postcard pack from <em>Public Domain Review</em></a></cite></p>
</blockquote>

    
    ]]>
</description>
<pubDate>Sat, 28 Oct 2017 06:46:01 -0400</pubDate>
<link>http://tinaja.computer/2017/10/28/cat-piano.html</link>
<guid isPermaLink="true">http://tinaja.computer/2017/10/28/cat-piano.html</guid>
</item>

<item>
<title>Connecting to Google Sheets with Python</title>

<description>
    <![CDATA[
    <p>I recently had cause to look into how difficult it would be to use a Python script to connect to a Google Sheets spreadsheet and pull down some data. I quickly found the <a href="https://github.com/burnash/gspread"><code class="highlighter-rouge">gspread</code></a> package which makes everything very simple. The most complicated part of the process is getting set up with authorization credentials to access the relevant spreadsheet remotely.</p>

<h2 id="the-problem">The Problem</h2>

<p>I’m working on a project that catalogs and maps live jazz performances around Austin, TX in the 1920s and 1930s. In these early stages, my partner on the project is doing a lot of the archival research and data collection while I work on developing scripts for processing our data and techniques we’ll use to present it on the web. We may eventually also have a research assistant or two, so having a distributed place to input our first-run archival notes is helpful.</p>

<p>So, we have the problem of wanting a ubiquitous and low-friction place for us to input our data while also needing that data to be well-structured so that it can be aggregated according to venue, date, performers, etc.</p>

<p>For ubiquity and low friction, we’re using Google Sheets for input. As a web-based tool, it is available anywhere for input, whether at library workstations or our personal machines. As a simple spreadsheet, data entry  mostly involves typing notes naturally,  within the limits of a few light formatting conventions, rather than futzing with a more structured interface.</p>

<p>To be able to use the data, we’ll want it rationalized into some predictably structured JSON files against which I can write scripts that will aggregate, map, and visualize these performance data in different ways.</p>

<p>Using <code class="highlighter-rouge">gspread</code> gave us a very simple, Pythonic way to get stuff out of the Google Sheets spreadsheet and into a form that we could work with easily in Python.</p>

<h2 id="configuring">Configuring</h2>

<p><code class="highlighter-rouge">gspread</code> provides the ability to connect to Google Sheets using a username and password, but particularly on a shared project, it’s going to be safer to use Google’s service account keys. The interface Google provides for setting up new credentials is not the simplest or most intuitive I’ve seen, but you only need to do it once.</p>

<p>First, sign in to the <a href="https://console.developers.google.com">Google Developers Console</a> and select ‘Credentials’ from the sidebar.</p>

<p>Clicking the ‘Create Credentials’ button will display a dropdown menu from which you will select ‘Service account key’. On the following screen, you’ll want to use the dropdown menu to create a new service account for which you’ll need to choose a Name and a Role. The name can be whatever makes the most sense for your project, and the Role should be ‘Project’ → ‘Owner’. Once you’ve configured these settings, you can download the generated JSON file.</p>

<p>The file you’ve just downloaded contains information that will provide unrestricted access to your spreadsheet, so keep it out of public source control repositories and the like.</p>

<p>When you open the JSON file, you will see a line that will look something like:</p>

<div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s2">"client_email"</span><span class="p">:</span> <span class="s2">"test-147@centering-abode-111415.iam.gserviceaccount.com"</span>
</code></pre></div></div>

<p>You’ll give your new service account access by going to the spreadsheet and using the ‘Share’ button in the upper right to share the spreadsheet with the email address listed on the <code class="highlighter-rouge">"client_email"</code> line.</p>

<h2 id="authorizing">Authorizing</h2>

<p>We’ll need to import <code class="highlighter-rouge">gspread</code> to work with our spreadsheet, and to authorize for access to Google from our script, we’ll need to import <code class="highlighter-rouge">ServiceAccountCredentials</code> from the <code class="highlighter-rouge">oauth2client</code> package.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">gspread</span>
<span class="kn">from</span> <span class="nn">oauth2client.service_account</span> <span class="kn">import</span> <span class="n">ServiceAccountCredentials</span>
</code></pre></div></div>

<p>Then we’ll authorize with Google using the JSON key file we downloaded, open our spreadsheet using its URL, and use <code class="highlighter-rouge">gspread</code> to download the spreadsheet worksheet in which we’re interested into a Worksheet object.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">credentials</span> <span class="o">=</span> <span class="n">ServiceAccountCredentials</span><span class="o">.</span><span class="n">from_json_keyfile_name</span><span class="p">(</span>
                <span class="n">path_to_keyfile</span><span class="p">,</span>
                <span class="p">[</span><span class="s">'https://spreadsheets.google.com/feeds'</span><span class="p">])</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">gspread</span><span class="o">.</span><span class="n">authorize</span><span class="p">(</span><span class="n">credentials</span><span class="p">)</span>

<span class="n">sheet</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">open_by_url</span><span class="p">(</span><span class="n">url_for_google_sheet</span><span class="p">)</span>
<span class="n">worksheet</span> <span class="o">=</span> <span class="n">sheet</span><span class="o">.</span><span class="n">get_worksheet</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<p>In <code class="highlighter-rouge">worksheet</code>, we now have an object with which we can manipulate all of the rows in our shared spreadsheet.</p>

<h2 id="ingesting-and-processing">Ingesting and Processing</h2>

<p>Our sheet has nine columns including some specifics regarding the performance, metadata related to the archival source, which person made the record, and a column called ‘Processed’ that this script will mark to indicate that it has ingested a record. This Processed column allows me to set up some conditional formatting rules in our spreadsheet that displays the lines in the database that have been processed by this script in grey text and struck through so that we can retain our data in its entirety as it was originally entered while minimizing visual distraction when inputting new material.</p>

<p>Our ingestion script will go through the rows that have not been marked as processed, add those rows to a list to be saved as JSON, and add the indexes of any rows we have processed to another list that will allow us to clean up after ourselves and mark newly processed rows in the shared spreadsheet.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">all_rows</span> <span class="o">=</span> <span class="n">worksheet</span><span class="o">.</span><span class="n">get_all_records</span><span class="p">()</span>

<span class="n">performances</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">processed_rows</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">all_rows</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">row</span><span class="p">[</span><span class="s">'Processed'</span><span class="p">]</span> <span class="o">!=</span> <span class="s">'*'</span><span class="p">:</span>
        <span class="n">keys</span> <span class="o">=</span> <span class="p">(</span><span class="s">'Artist'</span><span class="p">,</span> <span class="s">'Date'</span><span class="p">,</span> <span class="s">'Venue'</span><span class="p">,</span> <span class="s">'Source'</span><span class="p">,</span>
                <span class="s">'Notes'</span><span class="p">,</span> <span class="s">'Important'</span><span class="p">,</span> <span class="s">'Ad'</span><span class="p">,</span> <span class="s">'Contributor'</span><span class="p">)</span>

        <span class="c"># Create new Dict with only keys from list (Drop 'Processed')</span>
        <span class="n">new_row</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">row</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">keys</span><span class="p">}</span>
        <span class="n">performances</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_row</span><span class="p">)</span>

        <span class="c"># add 2 to index to skip header row *and* convert from</span>
        <span class="c"># zero-based indexing to 1-based</span>
        <span class="n">processed_rows</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">idx</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span>
</code></pre></div></div>

<p>The <code class="highlighter-rouge">Worksheet</code> model in <code class="highlighter-rouge">gspread</code> has a method <code class="highlighter-rouge">get_all_records()</code> that will give us all of the rows in our sheet. We then iterate through those rows, taking both the row’s index and the data from the row as a <code class="highlighter-rouge">Dict</code> where the keys are the titles of the columns in our spreadsheet.</p>

<p>If the Processed column on the row does not have our <code class="highlighter-rouge">*</code> character indicating that we have processed it before, we make a new <code class="highlighter-rouge">Dict</code> with the row data (excluding our Processed column) and add it to a list of <code class="highlighter-rouge">performances</code> to be saved, and add the index of the row to a list of <code class="highlighter-rouge">processed_rows</code> to be marked in the spreadsheet.</p>

<h2 id="finishing-up">Finishing Up</h2>

<p>The output of this script just saves off this data to be processed further at another time. That processing step is more manual, so we might be working with a specific generation of ingested data over multiple sessions. We name our output files with a unique timestamp to mark the generations.</p>

<p>First, we output the performance data itself.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Get a timestamp to mark output files uniquely</span>
<span class="n">ts</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">st</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="o">.</span><span class="n">fromtimestamp</span><span class="p">(</span><span class="n">ts</span><span class="p">)</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s">'</span><span class="si">%</span><span class="s">Y-</span><span class="si">%</span><span class="s">m-</span><span class="si">%</span><span class="s">d-</span><span class="si">%</span><span class="s">H-</span><span class="si">%</span><span class="s">M-</span><span class="si">%</span><span class="s">S'</span><span class="p">)</span>

<span class="c"># Output JSON for ingested data</span>
<span class="n">output_file</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s">'{}-performances.json'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">st</span><span class="p">),</span> <span class="s">'w'</span><span class="p">)</span>
<span class="n">output_file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">performances</span><span class="p">))</span>
</code></pre></div></div>

<p>And then output the list of row indexes that have been processed in case we need this history to update the Processed status in our spreadsheet manually.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Output row numbers for processed data in case automated</span>
<span class="c"># spreadsheet update fails</span>
<span class="n">output_file</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s">'{}-processed-rows.json'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">st</span><span class="p">),</span> <span class="s">'w'</span><span class="p">)</span>
<span class="n">output_file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">processed_rows</span><span class="p">))</span>
</code></pre></div></div>

<p>And, finally, we step through that list of row indexes and use <code class="highlighter-rouge">gspread</code> to put a star in the Processed column on each of our rows processed in this session, so that they will appear greyed out in our spreadsheet and will be ignored on future runs of this ingestion script.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">processed_rows</span><span class="p">:</span>
    <span class="n">worksheet</span><span class="o">.</span><span class="n">update_cell</span><span class="p">(</span><span class="n">row</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="s">'*'</span><span class="p">)</span>
</code></pre></div></div>

<p>These <code class="highlighter-rouge">update_cell()</code> calls happen one by one, each with an individual request to the server. Not only does this make the process relatively slow, but it also makes it relatively fragile. If the script hangs because of poor connectivity somewhere during the process of updating dozens (or hundreds) of individual rows in spreadsheet, we have our record of rows that we processed in this session saved off to a JSON file to help us update the spreadsheet manually if necessary.</p>

<h2 id="next-steps">Next Steps</h2>

<p>This is a small script with two purposes: to save data from a spreadsheet shared on Google Sheets into a local JSON file and to update the shared spreadsheet to reflect the fact that the data has been processed. Now that we have the data locally we can do all sorts of things to organize, structure, and aggregate it in ways that make it more useful to us while not having sacrificed the convenience of data entry with Google Sheets. These other scripts will do things like sanitize and standardize Venue and Performer names, establish relationships between connected Performers and Venues, and archive research notes so they can be called up in connection to a particular performance later.</p>


    
    ]]>
</description>
<pubDate>Fri, 27 Oct 2017 11:02:01 -0400</pubDate>
<link>http://tinaja.computer/2017/10/27/gspread.html</link>
<guid isPermaLink="true">http://tinaja.computer/2017/10/27/gspread.html</guid>
</item>

<item>
<title>Birthday Conservas</title>

<description>
    <![CDATA[
    <p>My parents were in Porto recently and got me some conservas from Lucas for my birthday. I love the packaging.</p>

<ul>
  <li>Filetes de atum em azeite de oliveira
<img src="/images/2017/conservas-atum.jpg" alt="Filetes de atum" /></li>
  <li>Filetes de chaputa em escabeche
<img src="/images/2017/conservas-chaputa.jpg" alt="Filetes de chaputa" /></li>
  <li>Peixe espada preto em filetes em azeite de oliveira
<img src="/images/2017/conservas-peixe-espada.jpg" alt="Peixe espada" /></li>
  <li>Sardinhas portuguesas em azeite de oliveira e piri-piri
<img src="/images/2017/conservas-sardinhas-piri-piri.jpg" alt="Sardinhas em azeite de oliveira piri-piri" /></li>
  <li>Sardinhas portuguesas em azeite de oliveira e pimento verde
<img src="/images/2017/conservas-sardinhas.jpg" alt="Sardinhas em azeite de oliveira e pimento verde" /></li>
</ul>

    
    ]]>
</description>
<pubDate>Mon, 23 Oct 2017 16:51:01 -0400</pubDate>
<link>http://tinaja.computer/2017/10/23/birthday-conservas.html</link>
<guid isPermaLink="true">http://tinaja.computer/2017/10/23/birthday-conservas.html</guid>
</item>

<item>
<title>Recent Reading: October 19, 2017</title>

<description>
    <![CDATA[
    

    
        


  <div class="clear-block"></div>
  <h3>Books</h3>
  <ul class="reading-list">
    
      <li>
        <span class="citation">
      <a href="http://www.amazon.com/exec/obidos/asin/B074TJ8M94/ref=nosim/latin031-20">Eva Holland. <i>Mussolini’s Arctic Airship</i>. Seattle, WA: Amazon Kindle Singles, 2017.</a>
      </span>

        
          <span class="note"><p>Holland centers the story of the wreck of the airship <em>Italia</em> over the Arctic on the Norwegian explorer Roald Amundsen and the path that put him on the rescue mission that ended his life. Produced as a Kindle Single, the work is compact, but alternates capably between the long historical trajectory of Amundsen’s career, the European geopolitics of the interwar period, and experience of the wrecked crew of the <em>Italia</em>. In particular, the narrative of the wreck provides a good example of creating narrative tension in moments of peril from relatively limited and contradictory primary sources.</p>
</span>
        <span class="links">
      <a href="http://www.amazon.com/exec/obidos/asin/B074TJ8M94/ref=nosim/latin031-20">Amazon &raquo;</a>
      </span>

      </li>
    
      <li>
        <span class="citation">
      <a href="http://www.amazon.com/exec/obidos/asin/1302900536/ref=nosim/latin031-20">Ta-Nehisi Coates and Brian Stelfreeze. <i>Black Panther: A Nation Under Our Feet, vol. 1</i>. New York: Marvel, 2016.</a>
      </span>

        
          <span class="note"><p>This opening volume of Ta-Nehisi Coates’s take on Black Panther was both impressive and a bit difficult to get into. It took me until the final issue of this collection to feel like I was on firm footing with the several storylines (and their different scales) going on. The story is very much embedded in our current historical moment of re-examination of our history of racial conflict, populist discontent, and armed conflict with non-state actors while also being engaged with comic book history in its skepticism with regard to the singular hero. From a packaging standpoint, including early appearances of Black Panther is a nice touch, and I hope that continues in future collections.</p>
</span>
        <span class="links">
      <a href="http://worldcat.org/oclc/968293730">WorldCat &raquo;</a>
      <a href="http://www.amazon.com/exec/obidos/asin/1302900536/ref=nosim/latin031-20">Amazon &raquo;</a>
      </span>

      </li>
    
      <li>
        <span class="citation">
      <a href="http://www.amazon.com/exec/obidos/asin/1302900544/ref=nosim/latin031-20">Ta-Nehisi Coates and Chris Sprouse. <i>Black Panther: A Nation Under Our Feet, vol. 2.</i> New York: Marvel, 2017.</a>
      </span>

        
          <span class="note"><p>I read this second volume straight on from the first, and much of my reaction to this second installment is the same. The main change is the inclusion of ‘The Crew’ comprised of Storm, Luke Cage, Manifold, and Misty Knight which provides some opportunity for some lighter dialogue to break up the heavy monologuing of the first volume of stories.</p>
</span>
        <span class="links">
      <a href="http://worldcat.org/oclc/951950785">WorldCat &raquo;</a>
      <a href="http://www.amazon.com/exec/obidos/asin/1302900544/ref=nosim/latin031-20">Amazon &raquo;</a>
      </span>

      </li>
    
      <li>
        <span class="citation">
      <a href="http://www.amazon.com/exec/obidos/asin/0143127438/ref=nosim/latin031-20">Paul Greenberg. <i>American Catch: The Fight for Our Local Seafood</i>. New York: The Penguin Press, 2014.</a>
      </span>

        
          <span class="note"><p>Paul Greenberg takes three sea creatures—the Eastern Oyster, the Gulf Shrimp, and the Alaskan Salmon—that typify North Americans’ historical engagement with seafood and uses them as a lens on the problems posed for maintaining seafood resources in the twenty-first century. Each serves as a different cautionary tale: the Eastern Oyster for over-development of our coastlines, the Gulf Shrimp for competition from globalized commodity markets built on aquaculture, and the Alaskan Salmon for American ignorance of the source of their seafood. The book is brisk and readable despite the bleak picture it paints and touches on a wide array of subjects including global labor markets, climate change, urban planning, and consumer tastes. Greenberg concludes in outlining some possibilities for progress and makes the case that positive change has already begun, but the changes are still small and the road ahead seems very, very long.</p>
</span>
        <span class="links">
      <a href="http://worldcat.org/oclc/921934890">WorldCat &raquo;</a>
      <a href="http://www.amazon.com/exec/obidos/asin/0143127438/ref=nosim/latin031-20">Amazon &raquo;</a>
      </span>

      </li>
    
  </ul>

    
    ]]>
</description>
<pubDate>Thu, 19 Oct 2017 10:10:00 -0400</pubDate>
<link>http://tinaja.computer/2017/10/19/recent-reading.html</link>
<guid isPermaLink="true">http://tinaja.computer/2017/10/19/recent-reading.html</guid>
</item>

<item>
<title>Expedience in Sourdough</title>

<description>
    <![CDATA[
    <p><a href="http://amzn.to/2ggt6NK"><img src="/images/2017/sourdough-cover.jpg" class="book-cover" alt="Sourdough cover" /></a></p>

<p>Robin Sloan sets his recent novel <a href="http://amzn.to/2ggt6NK"><em>Sourdough</em></a> in a familiar present or near-future San Francisco where startup culture sits uneasily on top of previous incarnations of the city’s many subcultures. The portrayal is a nuanced mix of sympathy and critique that can only come through familiarity. It is enthusiastic, but skeptical.</p>

<p>One small choice struck me in connection to this balance, and I think Sloan’s solution to this problem provides an interesting perspective on the role of online, networked defaults in our lives.</p>

<p>With the proliferation of corporate-owned, private online platforms and tools and their de facto status as default services, verisimilitude in fiction requires some acknowledgment that these branded services exist, but aesthetics or politics may compel us not to use specific names.</p>

<p>Sloan’s characters use “an <strong>expedient</strong> online retailer” or “the <strong>expedient</strong> image-based social network”. This construction sidesteps the datedness of brandname references while acknowledging the ubiquity of these services. More interestingly, I think, it deflates the power of the brands in a more subtle way than some of the more stridently anti-corporate formulations I have seen. It says “we have come to rely on the services these companies provide, but we need not care if these specific instances survive.”</p>

<p>This concept of ‘expedience’ encapsulates a lot of my recent hesitation around closed, branded, and/or privately-owned online services. So much of their strength comes from their convenience and ubiquity. They are the choice of people who don’t have the time or inclination to dig deeper in that area; they are sometimes actually better or even exceptional, but frequently they are simply expedient.</p>

<p>My primary reaction to this technique while reading was on these aesthetic grounds, but it strikes me that there is also some product design guidance here as well. People making services that want to deliver more to those who care need to focus on being sufficiently permeable to the expedient choice to defray the cost of connoisseurship. Expedience can trump excellence even for enthusiasts when it comes to networked platforms.Manton Reece’s goals for <a href="http://micro.blog">micro.blog</a>, for example, seem to track with this, though he still has a long way to go to make it a reality. It’s something I intend to keep in mind while I think about product ideas that are services in areas with existing expedient choices. I am happy to make things that addresses niches that appeal to specific tastes and interests, but I need to be careful not to make it too difficult to balance peculiar enthusiasms with expedient pragmatisms.</p>

    
    ]]>
</description>
<pubDate>Fri, 13 Oct 2017 13:05:01 -0400</pubDate>
<link>http://tinaja.computer/2017/10/13/expedience.html</link>
<guid isPermaLink="true">http://tinaja.computer/2017/10/13/expedience.html</guid>
</item>

<item>
<title>Famous Tex-Mex Quesos</title>

<description>
    <![CDATA[
    <p>I pre-ordered <a href="http://www.homesicktexan.com">Lisa Fain</a>’s new <a href="http://amzn.to/2y67xqu"><em>Queso!</em></a> cookbook months ago when I heard about it and it arrived today. I opened it to this list of five notable Tex-Mex restaurant quesos.</p>

<p>This list is correct. I am enthusiastic about the book.</p>

<ul>
  <li><strong>Bob Armstrong Dip</strong> – Matt’s El Rancho – Austin, TX</li>
  <li><strong>Felix Queso</strong> – Felix’s (and El Patio) – Houston, TX</li>
  <li><strong>José’s Dip</strong> – Molina’s - Houston, TX</li>
  <li><strong>Kerbey Queso</strong> – Kerbey Lane – Austin, TX</li>
  <li><strong>Mag Mud</strong> – Magnolia Cafe – Austin, TX</li>
</ul>


    
    ]]>
</description>
<pubDate>Tue, 26 Sep 2017 16:37:01 -0400</pubDate>
<link>http://tinaja.computer/2017/09/26/famous-tex-mex-quesos.html</link>
<guid isPermaLink="true">http://tinaja.computer/2017/09/26/famous-tex-mex-quesos.html</guid>
</item>

<item>
<title>Recent Reading: September 23, 2017</title>

<description>
    <![CDATA[
    <h3 id="links">Links</h3>

<ul>
  <li><a href="http://idlewords.com/2017/09/anatomy_of_a_moral_panic.htm"><strong>Anatomy of a Moral Panic—Idle Words—Maciej Ceglowski</strong></a>: Maciej is one of my favorite deflators of tech-industry-related hysterias and enthusiasms. His dissection of <a href="https://www.channel4.com/news/potentially-deadly-bomb-ingredients-on-amazon">this recent story</a> about Amazon shopping algorithms recommending bomb-making materials is a succinct indictment of the consequences of click-bait journalism and tech credulity.</li>
</ul>


    
        


  <div class="clear-block"></div>
  <h3>Books</h3>
  <ul class="reading-list">
    
      <li>
        <span class="citation">
      <a href="http://www.amazon.com/exec/obidos/asin/0785197028/ref=nosim/latin031-20">Ryan North and Erica Henderson. <i>The Unbeatable Squirrel Girl, vol. 1: Squirrel Power</i>. New York: Marvel, 2016.</a>
      </span>

        
          <span class="note"><p>I’m glad to have read this after taking a bit of a break from reading comics. The art style and writing tone are well-paired, and leaning into the silliness of the Squirrel Girl character design by having her face off with Galactus of all villains was a great instinct.</p>
</span>
        <span class="links">
      <a href="http://worldcat.org/oclc/962423243">WorldCat &raquo;</a>
      <a href="http://www.amazon.com/exec/obidos/asin/0785197028/ref=nosim/latin031-20">Amazon &raquo;</a>
      </span>

      </li>
    
      <li>
        <span class="citation">
      <a href="http://www.amazon.com/exec/obidos/asin/B00AXIZ464/ref=nosim/latin031-20">Tash Aw. <i>Five Star Billionaire: A Novel</i>. New York: Spiegel & Grau, 2014.</a>
      </span>

        
          <span class="note"><p>The Shanghai setting is as much a character as any of the five protagonists—all of them Malaysian transplants who followed different paths to China—that circle each other in this novel. The narrative is interspersed with passages from a self-help/business-achievement memoir that tracks the trajectories of each of the main characters and eventually exposes the isolation and dissatisfaction inherent in the lives prescribed by the worldview in these books and in the rapid urbanization and globalization of China. The end felt a bit pat. I would have accepted less wrapping up, but there were a lot of open threads to tie together.</p>
</span>
        <span class="links">
      <a href="http://worldcat.org/oclc/884930598">WorldCat &raquo;</a>
      <a href="http://www.amazon.com/exec/obidos/asin/B00AXIZ464/ref=nosim/latin031-20">Amazon &raquo;</a>
      </span>

      </li>
    
      <li>
        <span class="citation">
      <a href="http://www.amazon.com/exec/obidos/asin/B06XC41K6G/ref=nosim/latin031-20">Robin Sloan. <i>Sourdough: A Novel</i>. New York: MCD Books, 2017.</a>
      </span>

        
          <span class="note"><p>Robin Sloan may be my favorite active novelist. Reading <em>Sourdough</em> has made me want to pick up <a href="/2015/06/28/weekly-reading.html"><em>Mr. Penumbra’s 24-Hour Bookstore</em></a> for a third time. The quirky, questing enthusiasm of his characters and the narrow balance he strikes between small moments of magic tucked into a very recognizable quotidian present hit my pleasure-reading tastes squarely.</p>
</span>
        <span class="links">
      <a href="http://worldcat.org/oclc/968638708">WorldCat &raquo;</a>
      <a href="http://www.amazon.com/exec/obidos/asin/B06XC41K6G/ref=nosim/latin031-20">Amazon &raquo;</a>
      </span>

      </li>
    
  </ul>

    
    ]]>
</description>
<pubDate>Sat, 23 Sep 2017 18:10:00 -0400</pubDate>
<link>http://tinaja.computer/2017/09/23/recent-reading.html</link>
<guid isPermaLink="true">http://tinaja.computer/2017/09/23/recent-reading.html</guid>
</item>

<item>
<title>Columbus Podcasts</title>

<description>
    <![CDATA[
    <p>A list of podcasts associated with Columbus, OH that participated in the 2017 Independents’ Day Festival</p>

<ul>
  <li>Columbus! Something New</li>
  <li>ACES Radio</li>
  <li>Idea Foundry</li>
  <li>The Sounds of Bustown</li>
  <li>The Confluence Cast</li>
  <li>What’s The Deal With Creativity</li>
</ul>

    
    ]]>
</description>
<pubDate>Thu, 21 Sep 2017 10:40:01 -0400</pubDate>
<link>http://tinaja.computer/2017/09/21/columbus-podcasts.html</link>
<guid isPermaLink="true">http://tinaja.computer/2017/09/21/columbus-podcasts.html</guid>
</item>

</channel>
</rss>