<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/">
<channel>
<title>La Tinaja</title>
<description>My name is Brian Jones. These are my notes on topics including books, publishing, tech, and history.</description>
<link>http://localhost:4000</link>
<atom:link href="http://tinaja.computer/feed.xml" rel="self" type="application/rss+xml" />

<item>
<title>Media about Cities and Sea Levels</title>

<description>
    <![CDATA[
    <p>I read Paul Greenberg’s <em><a href="http://www.amazon.com/exec/obidos/asin/0143127438/ref=nosim/latin031-20">American Catch</a></em> <a href="http://tinaja.computer/2017/10/19/recent-reading.html">a few months ago</a> and while I enjoyed the whole thing, the most striking section to me was the material about oysters and New York City. The combination of food history, the interface between built and natural environments, and the marine pushed a lot of my buttons.</p>

<p>A recent 99% Invisible episode entitled “<a href="https://99percentinvisible.org/episode/oyster-tecture/">Oyster-tecture</a>” takes the same material as a jumping off point to dive more specifically into the oyster reef construction efforts of one of Greenberg’s subjects. The episode is great—as 99% Invisible so often is—but I hope it will also encourage more people to read Greenberg’s book.</p>

<p>In a similar vein, another of my favorite podcasts Track Changes has an <a href="https://trackchanges.postlight.com/talking-with-kim-stanley-robinson-about-his-global-warming-epic-new-york-2140-127a72460550">interview episode</a> with Kim Stanley Robinson on which he discusses his novel <em><a href="http://www.amazon.com/exec/obidos/asin/B01KT7YTO6/ref=nosim/latin031-20">New York 2140</a></em> (which I still need to read) and speculated on the consequences of rising sea levels for the built environment and urban geography of New York City.</p>


    
    ]]>
</description>
<pubDate>Sun, 21 Jan 2018 00:00:00 -0500</pubDate>
<link>http://localhost:4000/2018/01/21/oyster-tecture.html</link>
<guid isPermaLink="true">http://localhost:4000/2018/01/21/oyster-tecture.html</guid>
</item>

<item>
<title>Recent Reading: CDs and Bottled Water</title>

<description>
    <![CDATA[
    

    
        


  <div class="clear-block"></div>
  <h3>Books</h3>
  <ul class="reading-list">
    
      <li>
        <span class="citation">
      <a href="http://www.amazon.com/exec/obidos/asin/1558328793/ref=nosim/latin031-20">Natalie Perry. <i>The Big Book of Paleo Slow Cooking: 200 Nourishing Recipes That Cook Carefree, for Everyday Dinners and Weekend Feasts</i>. Minneapolis, MN: Harvard Common Press, 2017.</a>
      </span>

        
          <span class="note"><p>I’m usually pretty skeptical of these tool-oriented cookbooks (e.g., the Dutch oven cookbook, the slow cooker cookbook, etc.) and the combination with paleo dieting is the second strike. However, it was on the recommendations table at the downtown library (which tends to have pretty good cookbook recommendations), so I picked it up. I’m not a paleo diet partisan, so I imagine I’ll ignore some of the more inconvenient substitutions made for more conventional ingredients, but I actually found a lot of the recipes here to be pretty promising. The spice profile on the ones I’ve tried has been on the mild side for my taste, but that’s an easy thing to address in future efforts.</p>
</span>
        <span class="links">
      <a href="http://worldcat.org/oclc/993767508">WorldCat &raquo;</a>
      <a href="http://www.amazon.com/exec/obidos/asin/1558328793/ref=nosim/latin031-20">Amazon &raquo;</a>
      </span>

      </li>
    
      <li>
        <span class="citation">
      <a href="http://www.amazon.com/exec/obidos/asin/B00FEZ24QO/ref=nosim/latin031-20">Sherman Alexie. <i>Indian Killer</i>. New York: Grove Press, 1996.</a>
      </span>

        
          <span class="note"><p>Most of my fiction reading these days happens on a backlit Kindle Paperwhite on a couch in a toddler’s room after lights out. I found this really engrossing in that context. I love Sherman Alexie generally, and it was interesting to see him do something of a genre fiction exercise that, not surprisingly, transcends the typical limitations of the psychological thriller genre.</p>
</span>
        <span class="links">
      <a href="http://worldcat.org/oclc/612843404">WorldCat &raquo;</a>
      <a href="http://www.amazon.com/exec/obidos/asin/B00FEZ24QO/ref=nosim/latin031-20">Amazon &raquo;</a>
      </span>

      </li>
    
      <li>
        <span class="citation">
      <a href="http://www.amazon.com/exec/obidos/asin/B01DOJCAGA/ref=nosim/latin031-20">Jason Aaron, Chris Bachalo, et al. <i>Doctor Strange, Vol. 1: The Way of the Weird</i>. New York: Marvel Worldwide, 2015.</a>
      </span>

        
          <span class="note"><p>I really like Jason Aaron’s <em>Scalped</em>, and I like the more mystical Marvel stuff like Doctor Strange and the Silver Surfer, so it’s no surprise that I enjoyed this. I was not familiar with Chris Bachalo, but his art actually turned out to be my favorite aspect of this book.</p>
</span>
        <span class="links">
      <a href="http://worldcat.org/oclc/917344058">WorldCat &raquo;</a>
      <a href="http://www.amazon.com/exec/obidos/asin/B01DOJCAGA/ref=nosim/latin031-20">Amazon &raquo;</a>
      </span>

      </li>
    
      <li>
        <span class="citation">
      <a href="http://www.amazon.com/exec/obidos/asin/B072C8R69H/ref=nosim/latin031-20">Steve Knopper. <i>Appetite for Self-Destruction: The Spectacular Crash of the Record Industry in the Digital Age</i>. New York: Free Press, 2009.</a>
      </span>

        
          <span class="note"><p>This is an insightful history of the media trajectory of the music industry that created the anomalous peak of the CD era and the subsequent crash into the era of online distribution. Having come of age as a music consumer basically in lockstep with the rise of the CD format over the course of the 90s, I have been intellectually aware of the business distortions of the CD era, but my personal experience has made it difficult to relate to it. This book provided helpful perspective and historical distance in that regard. It was also fascinating to read this 2009 perspective in the year 2017 since streaming has taken off. In collecting bibliographic information for this title, I’ve seen that Knopper has had a chance to put out a new edition with a chapter on streaming this year. I’m curious to check it out, but also appreciate the first edition as a historical document.</p>
</span>
        <span class="links">
      <a href="http://worldcat.org/oclc/475718638">WorldCat &raquo;</a>
      <a href="http://www.amazon.com/exec/obidos/asin/B072C8R69H/ref=nosim/latin031-20">Amazon &raquo;</a>
      </span>

      </li>
    
      <li>
        <span class="citation">
      <a href="http://www.amazon.com/exec/obidos/asin/B004J4X32U/ref=nosim/latin031-20">Daniel J. Siegel and Tina Payne Bryson. <i>The Whole-Brain Child: 12 Proven Strategies to Nurture Your Child’s Developing Mind</i>. London: Robinson, 2012.</a>
      </span>

        
          <span class="note"><p>The authors’ approach in this parenting manual is to package research in developmental psychology for a popular audience. The result, for me, is a much more interesting read than a lot of “tips and tricks”-style manuals in this space, but it is still a bit weighed down by some of the repetitive tropes of the genre. The concept of integrating and accommodating opposing mental modes (individual vs. social, emotional vs. intellectual, etc.) into a whole has been a compelling goal in approaching both positive and negative behavior.</p>
</span>
        <span class="links">
      <a href="http://worldcat.org/oclc/989110062">WorldCat &raquo;</a>
      <a href="http://www.amazon.com/exec/obidos/asin/B004J4X32U/ref=nosim/latin031-20">Amazon &raquo;</a>
      </span>

      </li>
    
      <li>
        <span class="citation">
      <a href="http://www.amazon.com/exec/obidos/asin/1559707739/ref=nosim/latin031-20">Ismail Kadare. <i>The Successor: A Novel</i>. Translated by Tedi Papavrami and David Bellos. New York: Arcade Publishing, 2013.</a>
      </span>

        
          <span class="note"><p>I found this novel about the mysterious death of a high-ranking  government official in Albania to be an interesting exploration of political fear, paranoia, and suspicion in the context of an authoritarian regime. The language is stark and detached in a way that creates a sensation of alienation that I imagine to be intentional, but the English translation comes by way of a French translation of the original Albanian. I’m never confident that I’m not simply over-reading a difficult translation in cases like these. Despite taking a while to get into the language, I eventually quite enjoyed it.</p>
</span>
        <span class="links">
      <a href="http://worldcat.org/oclc/816030082">WorldCat &raquo;</a>
      <a href="http://www.amazon.com/exec/obidos/asin/1559707739/ref=nosim/latin031-20">Amazon &raquo;</a>
      </span>

      </li>
    
      <li>
        <span class="citation">
      <a href="http://www.amazon.com/exec/obidos/asin/B008ON449S/ref=nosim/latin031-20">Mohsin Hamid. <i>How to Get Filthy Rich in Rising Asia</i>. New York: Riverhead Books, 2014.</a>
      </span>

        
          <span class="note"><p>I enjoyed this quite a bit. It’s rather adventurous formally, presenting itself as a self-help book  with the goal of helping its reader achieve the titular goal of getting rich in the new Asia. The protagonist is the second person singular reader and the thematic chapters also trace the arc of that second person protagonist from birth through death in an unspecified, not-quite-Pakistan Asian country. The formal conceit may sound cute, but I found that it allowed Hamid to change frames from the intimately personal to the urban to the geopolitical freely. Having surrendered to the format quickly, I found it both touching and funny</p>
</span>
        <span class="links">
      <a href="http://worldcat.org/oclc/878603083">WorldCat &raquo;</a>
      <a href="http://www.amazon.com/exec/obidos/asin/B008ON449S/ref=nosim/latin031-20">Amazon &raquo;</a>
      </span>

      </li>
    
  </ul>

    
    ]]>
</description>
<pubDate>Wed, 10 Jan 2018 20:48:49 -0500</pubDate>
<link>http://localhost:4000/2018/01/10/recent-reading.html</link>
<guid isPermaLink="true">http://localhost:4000/2018/01/10/recent-reading.html</guid>
</item>

<item>
<title>Daniel Cook on ‘comfy games’</title>

<description>
    <![CDATA[
    <blockquote>
  <p>At a recent game designer retreat (Project Horseshoe) we had a working group trying to understand the psychology ‘comfy games’ like <em>Stardew Valley</em> or <em>Animal Crossing</em>. One thing we noticed is that many games focus on the bottom portion of Maslow’s Hierarchy of needs. They deal with survival, food, shelter and physical threats. Comfy games are instead comfortable spaces where you can work on some higher level human needs. You can dabble with self reflection, little social moments and maybe self expression. Games that treat players like higher order humans instead of twitchy, fear-based animals are a super cool thing!</p>

  <p>However, when a design introduces intense lower level needs, it can ruin the atmosphere that makes a comfy game work. <em>Pocket Camp</em> introduces time pressure and responsibility. If you don’t get that stupid sofa crafted before the animal leaves, you’ll miss a chance to progress along the checklist. Even when you do complete the task, it is entirely transactional. What would have been a mysterious quirky nuanced relationship in <em>Animal Crossing</em> is reduced to a meter incrementing so you can earn precious currency.  This is not an environment for self actualization. It is a place of rushed, time sensitive labor. Ugh.</p>

  <p>– <cite><a href="http://www.lostgarden.com/">Daniel Cook</a> on ‘comfy games’ at <a href="">Gamasutra</a></cite></p>
</blockquote>

    
    ]]>
</description>
<pubDate>Thu, 14 Dec 2017 08:58:01 -0500</pubDate>
<link>http://localhost:4000/2017/12/14/comfy-games.html</link>
<guid isPermaLink="true">http://localhost:4000/2017/12/14/comfy-games.html</guid>
</item>

<item>
<title>Sass Variables in CSS calc() Functions</title>

<description>
    <![CDATA[
    <p>The most common use cases for variables in Sass allow you to drop variables directly into Sass stylesheets, so it’s easy to forget (or not to know in the first place) that there are certain circumstances where the interpolation syntax is necessary. The one I always forget is to use interpolation inside of CSS functions, like <code class="highlighter-rouge">calc()</code>.</p>

<p>The concept of ‘<a href="https://en.wikipedia.org/wiki/String_interpolation">interpolation</a>’ just refers to the replacement of a placeholder with its literal value, so really in the case of processing Sass to CSS almost all of your references to variables are probably setting up an interpolation. That’s what’s happening when <code class="highlighter-rouge">color: $alertColor;</code> in your Sass gets changed to <code class="highlighter-rouge">color: #f00;</code> in your CSS output.</p>

<p>There are instances, however, in which the Sass preprocessor will not perform this substitution unless you <em>explicitly</em> specify interpolation using the <code class="highlighter-rouge">#{}</code> syntax. The <a href="http://sass-lang.com/documentation/file.SASS_REFERENCE.html#Interpolation_____">Sass documentation</a> explains that the interpolation syntax is necessary when using variables to construct selector and property names and may be used in property values except that it is only necessary in cases where operators next to it should be treated as plain CSS.</p>

<p>There is another case where the interpolation syntax is necessary, however, and it’s the one with which a recent (repeat) stumble inspired me to write up this note. Sass does not by default process the contents of CSS functions, so the line <code class="highlighter-rouge">min-height: calc($headerHeight - 2rem);</code> will be rendered to the CSS output with no change as <code class="highlighter-rouge">min-height: calc($headerHeight - 2rem);</code> (which is not valid CSS).</p>

<p>It is necessary to use interpolation explicitly to get the desire result in this case, so the correct line <code class="highlighter-rouge">min-height: calc(#{$headerHeight} - 2rem);</code> will produce CSS output such as <code class="highlighter-rouge">min-height: calc(50px - 2rem);</code> if the value of <code class="highlighter-rouge">$headerHeight</code> is <code class="highlighter-rouge">50px</code>.</p>

    
    ]]>
</description>
<pubDate>Sat, 02 Dec 2017 06:33:01 -0500</pubDate>
<link>http://localhost:4000/2017/12/02/sass-interpolation.html</link>
<guid isPermaLink="true">http://localhost:4000/2017/12/02/sass-interpolation.html</guid>
</item>

<item>
<title>Recent Reading: November 29, 2017</title>

<description>
    <![CDATA[
    

    
        


  <div class="clear-block"></div>
  <h3>Books</h3>
  <ul class="reading-list">
    
      <li>
        <span class="citation">
      <a href="http://www.amazon.com/exec/obidos/asin/0812975243/ref=nosim/latin031-20">Rob Gifford. <i>China Road: A Journey Into the Future of a Rising Power</i>. New York: Random House, 2008.</a>
      </span>

        
          <span class="note"><p>Rob Gifford had been an NPR China correspondent for years when he decided to follow Route 312, the transcontinental highway that travels west and north from Shanghai all the way to the border with Uzbekistan, as his farewell to China. <em>China Road</em> is his travelogue organized both chronologically and geographically as he follows the road and thematically as the different people he encounters provide him opportunities to explore the capitalist awakening in China, political repression, internal migration, the colonial and imperial history of China in Tibet and the Uighur territories, the tension between the improved material circumstances of low-income Chinese and accelerating inequality of opportunity, and many other subjects. Written in 2007, it is a particularly interesting read with the benefit of a decade of hindsight on some of the potential trends he identifies.</p>
</span>
        <span class="links">
      <a href="http://worldcat.org/oclc/938007671">WorldCat &raquo;</a>
      <a href="http://www.amazon.com/exec/obidos/asin/0812975243/ref=nosim/latin031-20">Amazon &raquo;</a>
      </span>

      </li>
    
      <li>
        <span class="citation">
      <a href="http://www.amazon.com/exec/obidos/asin/0062645234/ref=nosim/latin031-20">Anthony Horowitz. <i>Magpie Murders</i>. New York: Harper Perennial, 2017.</a>
      </span>

        
          <span class="note"><p>A double-whodunit that wraps a more modern thriller set in the London publishing industry wrapped around a Agatha Christie-esque English countryside cozy mystery. I would not be surprised if there were a lot of genre-insider stuff that I missed, but I was nonetheless impressed by the technical challenge posed by the structure and Horowitz’s success in pulling it off.</p>
</span>
        <span class="links">
      <a href="http://worldcat.org/oclc/995336235">WorldCat &raquo;</a>
      <a href="http://www.amazon.com/exec/obidos/asin/0062645234/ref=nosim/latin031-20">Amazon &raquo;</a>
      </span>

      </li>
    
      <li>
        <span class="citation">
      <a href="http://www.amazon.com/exec/obidos/asin/1585675245/ref=nosim/latin031-20">P.G. Wodehouse. <i>Ring for Jeeves</i>. Woodstock, N.Y.: Overlook Press, 2004 [1953].</a>
      </span>

        
          <span class="note"><p><em>Ring for Jeeves</em> has Jeeves on loan to the Earl of Rowcester while Bertie Wooster is away at a boarding school for young nobles to learn the basic life skills now necessary given the exigencies of the post-war welfare state. Rowcester’s own financial straits lead him to moonlight as a horse-racing bookmaker with Jeeves as his clerk. They are forced to welsh on a bet, and hilarity ensues through layers of escalating misunderstanding. Falls right in line with format and expectations, but fun stuff.</p>
</span>
        <span class="links">
      <a href="http://worldcat.org/oclc/55045280">WorldCat &raquo;</a>
      <a href="http://www.amazon.com/exec/obidos/asin/1585675245/ref=nosim/latin031-20">Amazon &raquo;</a>
      </span>

      </li>
    
      <li>
        <span class="citation">
      <a href="http://www.amazon.com/exec/obidos/asin/1616951117/ref=nosim/latin031-20">Helene Tursten. <i>Detective Inspector Huss</i>. Translated by Steven T. Murray. New York: Soho Press, 2003.</a>
      </span>

        
          <span class="note"><p>This is the first installment in a Swedish series of police procedurals set in Göteborg and written in the 1990s. The whodunit aspects of it were compelling and well-executed and there was enough suggestion of the socially- and politically-aware themes that crime fiction is can be so well-suited to exploring that I intend to continue. It was particularly interesting to read 90s Scandinavian angst over misogyny and white nationalism as the social context for a detective story from a late 2017 U.S. perspective.</p>
</span>
        <span class="links">
      <a href="http://worldcat.org/oclc/635706658">WorldCat &raquo;</a>
      <a href="http://www.amazon.com/exec/obidos/asin/1616951117/ref=nosim/latin031-20">Amazon &raquo;</a>
      </span>

      </li>
    
  </ul>

    
    ]]>
</description>
<pubDate>Wed, 29 Nov 2017 10:00:01 -0500</pubDate>
<link>http://localhost:4000/2017/11/29/recent-reading.html</link>
<guid isPermaLink="true">http://localhost:4000/2017/11/29/recent-reading.html</guid>
</item>

<item>
<title>Explaining the Obvious</title>

<description>
    <![CDATA[
    <blockquote>
  <p>Be more explicit than you think is necessary, and you’ll nip misunderstandings in the bud. When everyone’s in the same room, it’s easier to notice an errant gesture that shows we’re not all on the same page. A remote team can only know what you say and type, so get everyone to agree that overexplaining won’t be taken as an insult. Don’t let it slip as the project goes on, either. “So what we’re saying is…” should be a common phrase throughout the life of the product.</p>

  <p>– <cite>Drew Bell on <a href="https://trackchanges.postlight.com/five-ways-to-maintain-a-strong-remote-product-team-5d430c164ddf">talking through the “obvious” on remote teams</a></cite></p>
</blockquote>

    
    ]]>
</description>
<pubDate>Thu, 09 Nov 2017 07:39:01 -0500</pubDate>
<link>http://localhost:4000/2017/11/09/explain-the-obvious.html</link>
<guid isPermaLink="true">http://localhost:4000/2017/11/09/explain-the-obvious.html</guid>
</item>

<item>
<title>Open a File in MultiMarkdown Composer from BBEdit</title>

<description>
    <![CDATA[
    <p>Sometimes I find myself doing more extensive writing in a Markdown file in BBEdit than I had anticipated and want to switch to a more focused environment. My preferred Markdown editing environment for longer writing sessions is MultiMarkdown Composer, so I’ve added an AppleScript to BBEdit’s scripts menu to open whatever file I’m currently working on in with that. The script follows and, as you will see, can be easily edited to work with any other text editor you prefer.</p>

<div class="language-applescript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">tell</span><span class="w"> </span><span class="nb">application</span><span class="w"> </span><span class="s2">"BBEdit"</span><span class="w">
    </span><span class="k">set</span><span class="w"> </span><span class="nv">markdownFile</span><span class="w"> </span><span class="k">to</span><span class="w"> </span><span class="nv">file</span><span class="w"> </span><span class="k">of</span><span class="w"> </span><span class="nb">front</span><span class="w"> </span><span class="na">window</span><span class="w">
</span><span class="k">end</span><span class="w"> </span><span class="k">tell</span><span class="w">

</span><span class="k">tell</span><span class="w"> </span><span class="nb">application</span><span class="w"> </span><span class="s2">"MultiMarkdown Composer"</span><span class="w">
    </span><span class="nb">open</span><span class="w"> </span><span class="nv">markdownFile</span><span class="w">
    </span><span class="nb">activate</span><span class="w">
</span><span class="k">end</span><span class="w"> </span><span class="k">tell</span><span class="w">
</span></code></pre></div></div>

<p>That’s it.</p>

<p>Save the file in <code class="highlighter-rouge">~/Library/Application Support/BBEdit/Scripts</code>. You can get to this folder from the Finder by holding the ‘Option’ key while selecting the ‘Go’ menu in order to expose the ‘Library’ item which is usually hidden. A new item with the name of whatever you’ve named your script file will now appear in BBEdit’s script menu (identified by the AppleScript paper scroll icon).</p>

<p>You can add another layer of convenience by adding a keyboard shortcut for this action using the ‘Keyboard’ pane in the System Preferences application. Having opened the ‘Keyboard’ pane, go to the ‘Shortcuts’ tab and select ‘App Shortcuts’ from the sidebar. Click the ‘+’ button below the list on the right side and select ‘BBEdit’ as the Application, type your menu’s title exactly as it appears in the application, and set your keyboard shortcut. I use <code class="highlighter-rouge">Shift+Cmd+M</code>.</p>

    
    ]]>
</description>
<pubDate>Thu, 02 Nov 2017 11:46:01 -0400</pubDate>
<link>http://localhost:4000/2017/11/02/multimarkdown-composer-from-bbedit.html</link>
<guid isPermaLink="true">http://localhost:4000/2017/11/02/multimarkdown-composer-from-bbedit.html</guid>
</item>

<item>
<title>Athanasius Kircher’s Cat Piano</title>

<description>
    <![CDATA[
    <blockquote>
  <p>Not so long ago, in order to dispel the melancholy of some great prince, a noted and ingenious actor constructed an instrument such as this. He took live cats all of different sizes, and shut them up in a kind of box especially made for this business, so that their tails, stuck through the holes, were inserted tightly into certain channels. Under these he put keys fitted with the sharpest points instead of mallets. Then he arranged the cats tonally according to their different sizes, so that each key corresponded to the tail of one cat, and he put the instrument prepared for the relaxation of the prince in a suitable place. Then when it was played, it produced such music as the voices of cats can produce. For when the keys, depressed by the fingers of the organist, pricked the tails of the cats with their points, they, driven to a rage, with miserable voices, howling now low, now high, produced such music made of the voices of the cat as would move men to laughter and even arouse shrews to dance.</p>

  <p>– <cite>Athanasius Kircher in <em>Musurgia Universalis</em> (1650), as translated by Frederick Baron Crane, as printed on the insert in the last <a href="https://publicdomainreview.org/support/">postcard pack from <em>Public Domain Review</em></a></cite></p>
</blockquote>

    
    ]]>
</description>
<pubDate>Sat, 28 Oct 2017 06:46:01 -0400</pubDate>
<link>http://localhost:4000/2017/10/28/cat-piano.html</link>
<guid isPermaLink="true">http://localhost:4000/2017/10/28/cat-piano.html</guid>
</item>

<item>
<title>Connecting to Google Sheets with Python</title>

<description>
    <![CDATA[
    <p>I recently had cause to look into how difficult it would be to use a Python script to connect to a Google Sheets spreadsheet and pull down some data. I quickly found the <a href="https://github.com/burnash/gspread"><code class="highlighter-rouge">gspread</code></a> package which makes everything very simple. The most complicated part of the process is getting set up with authorization credentials to access the relevant spreadsheet remotely.</p>

<h2 id="the-problem">The Problem</h2>

<p>I’m working on a project that catalogs and maps live jazz performances around Austin, TX in the 1920s and 1930s. In these early stages, my partner on the project is doing a lot of the archival research and data collection while I work on developing scripts for processing our data and techniques we’ll use to present it on the web. We may eventually also have a research assistant or two, so having a distributed place to input our first-run archival notes is helpful.</p>

<p>So, we have the problem of wanting a ubiquitous and low-friction place for us to input our data while also needing that data to be well-structured so that it can be aggregated according to venue, date, performers, etc.</p>

<p>For ubiquity and low friction, we’re using Google Sheets for input. As a web-based tool, it is available anywhere for input, whether at library workstations or our personal machines. As a simple spreadsheet, data entry  mostly involves typing notes naturally,  within the limits of a few light formatting conventions, rather than futzing with a more structured interface.</p>

<p>To be able to use the data, we’ll want it rationalized into some predictably structured JSON files against which I can write scripts that will aggregate, map, and visualize these performance data in different ways.</p>

<p>Using <code class="highlighter-rouge">gspread</code> gave us a very simple, Pythonic way to get stuff out of the Google Sheets spreadsheet and into a form that we could work with easily in Python.</p>

<h2 id="configuring">Configuring</h2>

<p><code class="highlighter-rouge">gspread</code> provides the ability to connect to Google Sheets using a username and password, but particularly on a shared project, it’s going to be safer to use Google’s service account keys. The interface Google provides for setting up new credentials is not the simplest or most intuitive I’ve seen, but you only need to do it once.</p>

<p>First, sign in to the <a href="https://console.developers.google.com">Google Developers Console</a> and select ‘Credentials’ from the sidebar.</p>

<p>Clicking the ‘Create Credentials’ button will display a dropdown menu from which you will select ‘Service account key’. On the following screen, you’ll want to use the dropdown menu to create a new service account for which you’ll need to choose a Name and a Role. The name can be whatever makes the most sense for your project, and the Role should be ‘Project’ → ‘Owner’. Once you’ve configured these settings, you can download the generated JSON file.</p>

<p>The file you’ve just downloaded contains information that will provide unrestricted access to your spreadsheet, so keep it out of public source control repositories and the like.</p>

<p>When you open the JSON file, you will see a line that will look something like:</p>

<div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s2">"client_email"</span><span class="p">:</span> <span class="s2">"test-147@centering-abode-111415.iam.gserviceaccount.com"</span>
</code></pre></div></div>

<p>You’ll give your new service account access by going to the spreadsheet and using the ‘Share’ button in the upper right to share the spreadsheet with the email address listed on the <code class="highlighter-rouge">"client_email"</code> line.</p>

<h2 id="authorizing">Authorizing</h2>

<p>We’ll need to import <code class="highlighter-rouge">gspread</code> to work with our spreadsheet, and to authorize for access to Google from our script, we’ll need to import <code class="highlighter-rouge">ServiceAccountCredentials</code> from the <code class="highlighter-rouge">oauth2client</code> package.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">gspread</span>
<span class="kn">from</span> <span class="nn">oauth2client.service_account</span> <span class="kn">import</span> <span class="n">ServiceAccountCredentials</span>
</code></pre></div></div>

<p>Then we’ll authorize with Google using the JSON key file we downloaded, open our spreadsheet using its URL, and use <code class="highlighter-rouge">gspread</code> to download the spreadsheet worksheet in which we’re interested into a Worksheet object.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">credentials</span> <span class="o">=</span> <span class="n">ServiceAccountCredentials</span><span class="o">.</span><span class="n">from_json_keyfile_name</span><span class="p">(</span>
                <span class="n">path_to_keyfile</span><span class="p">,</span>
                <span class="p">[</span><span class="s">'https://spreadsheets.google.com/feeds'</span><span class="p">])</span>
<span class="n">client</span> <span class="o">=</span> <span class="n">gspread</span><span class="o">.</span><span class="n">authorize</span><span class="p">(</span><span class="n">credentials</span><span class="p">)</span>

<span class="n">sheet</span> <span class="o">=</span> <span class="n">client</span><span class="o">.</span><span class="n">open_by_url</span><span class="p">(</span><span class="n">url_for_google_sheet</span><span class="p">)</span>
<span class="n">worksheet</span> <span class="o">=</span> <span class="n">sheet</span><span class="o">.</span><span class="n">get_worksheet</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<p>In <code class="highlighter-rouge">worksheet</code>, we now have an object with which we can manipulate all of the rows in our shared spreadsheet.</p>

<h2 id="ingesting-and-processing">Ingesting and Processing</h2>

<p>Our sheet has nine columns including some specifics regarding the performance, metadata related to the archival source, which person made the record, and a column called ‘Processed’ that this script will mark to indicate that it has ingested a record. This Processed column allows me to set up some conditional formatting rules in our spreadsheet that displays the lines in the database that have been processed by this script in grey text and struck through so that we can retain our data in its entirety as it was originally entered while minimizing visual distraction when inputting new material.</p>

<p>Our ingestion script will go through the rows that have not been marked as processed, add those rows to a list to be saved as JSON, and add the indexes of any rows we have processed to another list that will allow us to clean up after ourselves and mark newly processed rows in the shared spreadsheet.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">all_rows</span> <span class="o">=</span> <span class="n">worksheet</span><span class="o">.</span><span class="n">get_all_records</span><span class="p">()</span>

<span class="n">performances</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">processed_rows</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">row</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">all_rows</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">row</span><span class="p">[</span><span class="s">'Processed'</span><span class="p">]</span> <span class="o">!=</span> <span class="s">'*'</span><span class="p">:</span>
        <span class="n">keys</span> <span class="o">=</span> <span class="p">(</span><span class="s">'Artist'</span><span class="p">,</span> <span class="s">'Date'</span><span class="p">,</span> <span class="s">'Venue'</span><span class="p">,</span> <span class="s">'Source'</span><span class="p">,</span>
                <span class="s">'Notes'</span><span class="p">,</span> <span class="s">'Important'</span><span class="p">,</span> <span class="s">'Ad'</span><span class="p">,</span> <span class="s">'Contributor'</span><span class="p">)</span>

        <span class="c"># Create new Dict with only keys from list (Drop 'Processed')</span>
        <span class="n">new_row</span> <span class="o">=</span> <span class="p">{</span><span class="n">key</span><span class="p">:</span> <span class="n">row</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="n">keys</span><span class="p">}</span>
        <span class="n">performances</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_row</span><span class="p">)</span>

        <span class="c"># add 2 to index to skip header row *and* convert from</span>
        <span class="c"># zero-based indexing to 1-based</span>
        <span class="n">processed_rows</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">idx</span> <span class="o">+</span> <span class="mi">2</span><span class="p">)</span>
</code></pre></div></div>

<p>The <code class="highlighter-rouge">Worksheet</code> model in <code class="highlighter-rouge">gspread</code> has a method <code class="highlighter-rouge">get_all_records()</code> that will give us all of the rows in our sheet. We then iterate through those rows, taking both the row’s index and the data from the row as a <code class="highlighter-rouge">Dict</code> where the keys are the titles of the columns in our spreadsheet.</p>

<p>If the Processed column on the row does not have our <code class="highlighter-rouge">*</code> character indicating that we have processed it before, we make a new <code class="highlighter-rouge">Dict</code> with the row data (excluding our Processed column) and add it to a list of <code class="highlighter-rouge">performances</code> to be saved, and add the index of the row to a list of <code class="highlighter-rouge">processed_rows</code> to be marked in the spreadsheet.</p>

<h2 id="finishing-up">Finishing Up</h2>

<p>The output of this script just saves off this data to be processed further at another time. That processing step is more manual, so we might be working with a specific generation of ingested data over multiple sessions. We name our output files with a unique timestamp to mark the generations.</p>

<p>First, we output the performance data itself.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Get a timestamp to mark output files uniquely</span>
<span class="n">ts</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
<span class="n">st</span> <span class="o">=</span> <span class="n">datetime</span><span class="o">.</span><span class="n">datetime</span><span class="o">.</span><span class="n">fromtimestamp</span><span class="p">(</span><span class="n">ts</span><span class="p">)</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s">'</span><span class="si">%</span><span class="s">Y-</span><span class="si">%</span><span class="s">m-</span><span class="si">%</span><span class="s">d-</span><span class="si">%</span><span class="s">H-</span><span class="si">%</span><span class="s">M-</span><span class="si">%</span><span class="s">S'</span><span class="p">)</span>

<span class="c"># Output JSON for ingested data</span>
<span class="n">output_file</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s">'{}-performances.json'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">st</span><span class="p">),</span> <span class="s">'w'</span><span class="p">)</span>
<span class="n">output_file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">performances</span><span class="p">))</span>
</code></pre></div></div>

<p>And then output the list of row indexes that have been processed in case we need this history to update the Processed status in our spreadsheet manually.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Output row numbers for processed data in case automated</span>
<span class="c"># spreadsheet update fails</span>
<span class="n">output_file</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s">'{}-processed-rows.json'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">st</span><span class="p">),</span> <span class="s">'w'</span><span class="p">)</span>
<span class="n">output_file</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">json</span><span class="o">.</span><span class="n">dumps</span><span class="p">(</span><span class="n">processed_rows</span><span class="p">))</span>
</code></pre></div></div>

<p>And, finally, we step through that list of row indexes and use <code class="highlighter-rouge">gspread</code> to put a star in the Processed column on each of our rows processed in this session, so that they will appear greyed out in our spreadsheet and will be ignored on future runs of this ingestion script.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">for</span> <span class="n">row</span> <span class="ow">in</span> <span class="n">processed_rows</span><span class="p">:</span>
    <span class="n">worksheet</span><span class="o">.</span><span class="n">update_cell</span><span class="p">(</span><span class="n">row</span><span class="p">,</span> <span class="mi">9</span><span class="p">,</span> <span class="s">'*'</span><span class="p">)</span>
</code></pre></div></div>

<p>These <code class="highlighter-rouge">update_cell()</code> calls happen one by one, each with an individual request to the server. Not only does this make the process relatively slow, but it also makes it relatively fragile. If the script hangs because of poor connectivity somewhere during the process of updating dozens (or hundreds) of individual rows in spreadsheet, we have our record of rows that we processed in this session saved off to a JSON file to help us update the spreadsheet manually if necessary.</p>

<h2 id="next-steps">Next Steps</h2>

<p>This is a small script with two purposes: to save data from a spreadsheet shared on Google Sheets into a local JSON file and to update the shared spreadsheet to reflect the fact that the data has been processed. Now that we have the data locally we can do all sorts of things to organize, structure, and aggregate it in ways that make it more useful to us while not having sacrificed the convenience of data entry with Google Sheets. These other scripts will do things like sanitize and standardize Venue and Performer names, establish relationships between connected Performers and Venues, and archive research notes so they can be called up in connection to a particular performance later.</p>


    
    ]]>
</description>
<pubDate>Fri, 27 Oct 2017 11:02:01 -0400</pubDate>
<link>http://localhost:4000/2017/10/27/gspread.html</link>
<guid isPermaLink="true">http://localhost:4000/2017/10/27/gspread.html</guid>
</item>

<item>
<title>Birthday Conservas</title>

<description>
    <![CDATA[
    <p>My parents were in Porto recently and got me some conservas from Lucas for my birthday. I love the packaging.</p>

<ul>
  <li>Filetes de atum em azeite de oliveira
<img src="/images/2017/conservas-atum.jpg" alt="Filetes de atum" /></li>
  <li>Filetes de chaputa em escabeche
<img src="/images/2017/conservas-chaputa.jpg" alt="Filetes de chaputa" /></li>
  <li>Peixe espada preto em filetes em azeite de oliveira
<img src="/images/2017/conservas-peixe-espada.jpg" alt="Peixe espada" /></li>
  <li>Sardinhas portuguesas em azeite de oliveira e piri-piri
<img src="/images/2017/conservas-sardinhas-piri-piri.jpg" alt="Sardinhas em azeite de oliveira piri-piri" /></li>
  <li>Sardinhas portuguesas em azeite de oliveira e pimento verde
<img src="/images/2017/conservas-sardinhas.jpg" alt="Sardinhas em azeite de oliveira e pimento verde" /></li>
</ul>

    
    ]]>
</description>
<pubDate>Mon, 23 Oct 2017 16:51:01 -0400</pubDate>
<link>http://localhost:4000/2017/10/23/birthday-conservas.html</link>
<guid isPermaLink="true">http://localhost:4000/2017/10/23/birthday-conservas.html</guid>
</item>

</channel>
</rss>